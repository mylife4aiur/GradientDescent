## Nesterov Accelerated Gradient （NAG）
NAG能帮助我们实现这种能力。

传统的动量方法通过梯度项和动量项两部分对当前参数做出修改（移动位置）

如果只计算动量项，我们可以得到参数下一个位置的粗略的近似值。

NAG通过修改梯度项达到最终效果，NAG计算的不是当前位置的梯度，而是计算通过动量项得到的下一个参数近似位置的梯度。

NAG的这种策略能防止参数“走得太快”导致的高“敏感度”。达到了更好的表现。

## 小结及过渡

到现在，我们实现了根据目标函数形成的超平面，来更新我们的参数。实现了对梯度下降的加速。然而我们希望针对不同的参数按照其重要性的不同采取不同的更新策略。

## Adagrad
Adagrad实现了这样的作用：
调整参数的学习率(learning rate)p，为不经常出现的参数采用更大的更新，经常出现的采用较小的更新。

Adagrad极大地提高了SGD的robustness，被用在了google的 "*learned to recognize cats in Youtube videos*" 

前面我们同时对所有的参数进行一次更新，使用相同的学习率。


### 局限
Adagrad的主要局限在于其在分母上对平方梯度的累加，因为每一次累加项都是正的，所以最终学习率会变成无穷小，此时算法无法再学习到额外的知识了。

## Adadelta






$\lambda$